# -*- coding: utf-8 -*-
"""CIE 425 Part 2 Convolutional Encoding and Decoding.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1mRboS1k1eBMU1_-3NkeRIEqP4IoBZf8h

# **Convolutional Encoder and Decoder** 
In this notebook we implement a Convolutional encoder and decoder (**Viterbi Decoder**) as the phase 2 of of **CIE 425 : Information Theory and Coding** course project in **University of Science and Technology, Zewail City, Egypt**. 

The Algorithm is tested along with the entropy encoder/decoder (**Huffman**)  on a test .txt file while adding a noise to the transmitted sequence. 

### **Method** 
The Huffman encoder and decoder implementation is based on the tree data structure.
The Convolutional decoder is based on the Viterbi algorithm.

## **Team Members and Mails**

####Hazem Khairy  201700796
s-hazem_khairy@zewailcity.edu.eg
####Mahmoud Yasser  201700960
s-mahmoudyasser@zewailcity.edu.eg
####Abdelrahman Elbadrawy  201700952
s-a.khairy@zewailcity.edu.eg
"""

import numpy as np
import sympy
from sympy import Matrix

"""# Channel Coding
In order to enhance communication systems, there are 2 simultanueous approaches
1. Enhancing the physical system, pulse shaping, reducing ISI, finding optimum modulation method
2. Channel coding

Channel coding is a very powerful tool, since it improves performance of the system without doing anything in 1. This is very advantageuous, since it provides a venue reliable communication over channels that  would otherwise be unreliable

# Convolutional Code
Convolutional code is a form of Channel codes, where instead of appending some parity bits at the end of each codeword, the entire code is changed. 

This offers the possibility of having better code rates compared to block codes, at the same redundancy. 

The downside is that convolutional codes are more complex to decode

# Methodology
Convolution coding takes its name from convolution operation, where a window of length (k) is passed along the input. The result of this convolution is taken to mod 2.

For the same window, we can have multiple coefficients, each group is called (generating polynomial). 

The more generating polynomials we use, the more redundancy is added, and the more reliable the channel becomes, but the less the code rate

# Convolutional Encoder
"""

def conv_encode(messege,polynomials):
  """
  This function performs convolutional encoding using the input generating polynomials and targent message to be encoded 
  Arguments:
    message: The stream to be encoded
    polynomials: The generating polynomials of the convolutional code'
  Returns:
    The convolutional code of the input stream (message)
  """
  # converts the input to list of integers
  x=np.array( [int(s) for s in messege ])
  
  # add zeros to properly perform convolution at the beginning
  L=polynomials.shape[1] # [[1,1,1],[1,0,1],[0,1,1]]
  x=np.append(np.zeros(L-1),x)

  # performing the encoding 
  return np.array([[sum((g)*x[range(n+L-1,n-1,-1)])%2 for n in range(len(x)-L+1)  ] for g in  polynomials]).flatten().astype(int)

"""# Viterbi Decoding of Convolutional code
Now that we encoded our data using convolutional code, and some noise was added to it, it is time to decode. 

#### **A brute force approach would be trying to**
1. Search all possible combinations of the encoded stream, and pick the one with smallest Hamming distance with the received stream
2. Decode the new found closest combination into meaningful message

However, this approach is very consuming in terms of both memory and processing, to the extent that the problem can be intractable.

#### **There is a better algorithm for decoding, called the Viterbi decoding algorithm.**
In this algorithm, we construct a trellis showing the evolution of the code state with time. The next figure shows 
<center> <img src ="https://image.slidesharecdn.com/presentationconvolutionerrorcontrolcoding-160701041407/95/convolutional-error-control-coding-26-638.jpg?cb=1467346529"> 
</center>


In the beginning, all boxes are empty, excpet for the boxes at the first column. After that, we do 2 steps
1. Forwards pass: In this step, the trellis is iteratively updated with the cost associated with traversing the trellis, where
  1. For every received codeword, calculate the hamming distance between this codeword and the possible states from the current column
  2. Add this hamming distance to the current cost in the current column
  3. Update each state in the next column by chossing the minimum incremented cost from a previous possible state.

2. Now, that the trellis is complete, we can choose the minimum cost in the last layer. This number is in fact the Hamming distance between the received stream and the transmitted stream. Now that we have this number, we can backtrack to find the path that led to it, and this constitutes the second part of the algorithm, the backward pass

For the Hamming distance, we can perform a simple bitwise XOR operation


- -- If the two bits are the same, XOR will be 0
- -- If the two bits are different, XOR will be 1

Adding the resulting bits will yield the number of mismatched bits (Hamming distance)
"""

def ham_dis(c1,c2):
    "calculate the hamming Distance between two bit streams "
    return sum(np.logical_xor(c1,c2))

ham_dis([0,0,1],[0,0,0])

"""## Implementation of Viterbi Convolutional Decoder

### Finite State Machine Dictionary

For this, we will use the `state_machine_generator(polynomials)` and its helper functions which will return a dictionary (key/value) pair where:
- > The key: [incoming bit,  current state]
- > The value: [next state, output]

The advantage is that we will generate this dictionary and store it in a variable in order to avoid unnecessary calculations in the Virtebri decoder function.
"""

def gen_states(states,n,temp=[]):
    """
    This piece of code generates a binary scale 
    For example if n = 4 it will fill the empty list states with
    00 01 10 11 sequentially
    """
    temp1=temp.copy()
    temp2=temp.copy()
    if n==0:
        states.append(temp)
        return
    temp1.append(0)
    gen_states(states,n-1,temp1)
    temp2.append(1)
    gen_states(states,n-1,temp2)
states=[]
polynomials=np.array([[1,1,1],[1,1,0],[1,0,1]])
gen_states(states,len(polynomials[0])-1)
states=np.array(states)
states

def b2d(binary):
    """
    converts from binary to decimal
    """
    return int("".join(str(x) for x in binary), 2)


def state_machine(bit_in,state,polynomials):
    """
    A Helper Function that returns the output and next state of finite state machine (defined by its polynomials) 
    given the current state of the machine and the input. 
    """
    # get the next state
    next_state=np.append(bit_in,state)

    # get the output of the current state
    out=np.array([ sum(next_state*p)%2 for p in polynomials ]) 
    return next_state[:-1],out


def state_machine_generator(polynomials):
    """
    Returns a dictionary of behavior of the state machine (defined by its polynomials).
    The dictionary returns the the output and next state given the input and the current state
    """
    states=[]
    gen_states(states,len(polynomials[0])-1)
    return {str([bit_in,state]):state_machine(bit_in,state,polynomials) for bit_in in [0,1] for state in states}

state_machine_generator(polynomials)

"""### Forward pass of the trellis
For the Trellis forward pass, we will need to do the following for each column:
1. Identify the possible next states to move from.
2. Update the error cost for the next states
"""

def update_next(states,current_state,current_cost,dic_sm,next_states,recived_parity):
    """
    This functions is used a part of the viterbi algorithm in filling the error cost of the trellis.
    Updates the next coloumn of the trellis based on the current coloumn. 
    Inputs: 
      states: all possible states
      current_state: index of the current state that would be used to update the costs of the next coloumn of the trellis
      dic_sm: State machine dictionary
      next_states: a reference to the next coloumn of the trellis
      recived_parity: the recived parity
    """
    next_state_0,out_0=dic_sm[str([0,states[current_state]])] # calculate the output(to get the branch metric) and the next state in case of zero 
    next_state_1,out_1=dic_sm[str([1,states[current_state]])] # calculate the output(to get the branch metric) and the next state in case of zero 
    next_state_0=b2d(next_state_0) # converts to decimal to use it as index
    next_state_1=b2d(next_state_1) # converts to decimal to use it as index

    # The next two lines updates the two possible next states emerging from the current state of the trellis
    # It does so by updating the next state with the minium of the next state itself and the error cost of the current state + the branch metric 
    next_states[next_state_0]=min(next_states[next_state_0],current_cost+ham_dis(out_0,recived_parity)) 
    next_states[next_state_1]=min(next_states[next_state_1],current_cost+ham_dis(out_1,recived_parity))
    return

"""### Backward pass of the trellis
After the forward pass, and the trellis is updated, we can identify the tail of the best path (least cost in the last column). We will then move backwards beginning from this tail, and thus we get the path with least cost.<br>

While moving backwards, we must restrict our search space to the possible states that led to our current state. Failing to do so may lead to errors. 
"""

def get_previous(states,current_state,previous_states):
    """
    This function is used inside the viterbi decoding to get the final stream of the decoded bits.
    The functions returns the output bit the previous state in the trellis while propagates the trellis backwards.
    inputs:
        states: all possible states
        current_state: index of the current state that would be used to get the output bit and previous state in the trellis
        previous_states: a reference to the previous coloumn of the trellis (containing the error costs)
    returns:
      - The index of the previous state
      - The output bit
    """
    pre_0 = states[current_state][1:] # getting possible previous states in case of input zero
    pre_1 = states[current_state][1:] # getting possible previous states in case of input one
    pre_0.append(0) # appending zero
    pre_1.append(1) # appending one
    pre_state=[pre_0,pre_1] # puting them in a list 
    pre=np.argmin([previous_states[b2d(pre_state[0])],previous_states[b2d(pre_state[1])]]) # getting the state with minimum cost
    return b2d(pre_state[pre]),states[current_state][0] # returns the index of the state with minimum cost and the decoded bit

"""### Putting it all in one function"""

def Viterbi(recived,polynomials):
    """
    The functions returns the decoded messege "out" based on the recived messeage and the generating polynomials as an inputs 
    """
     # reshaping the recived bits to mark each parity bits together 
    recived=recived.reshape( len(polynomials),int(len(recived)/len(polynomials)))

    # Generating the finite state machine dictionary to avoit repeating calculations 
    dic_sm=state_machine_generator(polynomials)  
    
    # initializing the trellis with infinity error costs 
    trellis=np.array([[np.Inf for i in range(2**(len(polynomials[0])-1))]]*(len(recived[0])+1)) 

    # initializing the very first state (zero state) of the trellis with
    trellis[0,0]=0  

    # Forward pass
    states=[]
    gen_states(states,len(polynomials[0])-1) # getting possible states of the trellis
    for i in range(len(recived[0])): # looping over the packaged parity bits  
        for j in range(len(states)): # looping over the states in the trellis coloumns
            update_next(states,j,trellis[i,j],dic_sm,trellis[i+1],recived[:,i]) # updating the next trellis coloumn using current state in the current coloumn 
    
    # backward pass
    out= np.zeros(len(trellis)-1) # creating memory for the output
    current_state=np.argmin(trellis[-1])  # getting the minimum path from the right end of the trellis
    for i in range(len(trellis)-1,0,-1):
        current_state,out[i-1]=get_previous(states,current_state,trellis[i-1]) # using current minimum states to get the decoded bit and the previous state
    return out

"""# Importing Entropy Encoder and decoder functions (form phase one of the project)"""

!gdown --id 12oyJQ9g9ai0a7a5YvPZ6SHvSP2lfpmAh
from phase_1_utils import *

"""# Building an end to end system
Now that we have can perform source coding, and channel coding, we can combine them both into a transmitter and a receiver, in an end-to-end system
### **A question we may ask is, how can we add AWGN to binary data**?
We can only add AWGN to continuous data. Therefore, we will assume that the modulation scheme is ASK (Amplitude Shift Keying), where $E_b$ is 1, for simplicity.


"""

def transmit(file_name, polys=polynomials, enc_channel = True, SNR = None):
  '''encodes the file contents using Huffman code, then performs channel convolutional channel coding
  Arguments: 
    file_name: The path of the file whose content we want to encode and transmit
    polys: The generating polynomials
    enc_channel: A flag whether to apply channel coding or not
    SNR: The signal to noise ratio. If none, no noise will be added

  Returns:
    noisy_signal: The binary data as received by a matched filter
    entropy_ints: The huffman encoded stream as ints
    huff_tree: The huffman tree generated for source encoding
    huff_dict: A dictionary of each source symbol to its huffman encoded symbol 
  '''
  # source coding
  entropy_encoded, huff_tree, huff_dict, symb_dict = encode_huffman(file_name)

  #convert to ints for a numerical array, since encode_huffman encodes stream in string format
  entropy_code_ints = [int(x) for x in entropy_encoded] 

  # channel coding
  if enc_channel:
    conv_encoded = conv_encode(entropy_code_ints, polys)
  else:
    conv_encoded = entropy_code_ints
  
  # noise parameters. If noise is not None
  if SNR is not None:
    Eb = 1
    N0 = Eb/np.power(10, SNR/10)
    variance = N0 / 2

    # generating noise and adding it
    noise = np.random.randn(len(conv_encoded)) * np.sqrt(variance)
    noisy_signal = noise + conv_encoded

    # thresholding with root(Eb)/2
    noisy_signal = np.array([int(x > 0.5) for x in noisy_signal])
  
  # if no noise
  else:
    noisy_signal = conv_encoded

  return noisy_signal, entropy_code_ints, huff_tree

def receive(stream, huff_tree, polys = polynomials, enc_channel = True):
  '''Decodes the channel code and the source coding
  Arguments: 
    stream: The bit stream as received
    polys: The generating polynomials used by the transmitter
    huff_tree: The huffman tree constructed during encoding
  Returns:
    source_decode: The original message decoded (source decoding)
    channel_decode: The channel decoding
  '''
  
  # conv => Huff
  if enc_channel:
    channel_decode = Viterbi(stream, polys)
  else:
    channel_decode = stream

  # convert to string representation
  channel_decode_str = channel_decode.astype(int).astype(str)

  # Huff => letters
  source_decode = huffman_decode(channel_decode_str, huff_tree)

  return source_decode, channel_decode

"""# Testing end to end system with noiseless channal"""

# transmitting
noiseless_sig, entropy_code_ints, huff_tree = (
    transmit("Test_text_file.txt", polynomials, enc_channel= True,SNR = None)
    )

# receiving
rec_message, channel_decode = receive(noiseless_sig, huff_tree, polynomials)
rec_message

# comparing
with open ("Test_text_file.txt") as file:
  original = file.read()
print("Is the orignal the same as the received? ")
print(original == rec_message)

"""The messege is prefectly decoded

# Testing end to end system with noisy channal
"""

# transmitting
noisy_sig, entropy_code_ints, huff_tree = (
    transmit("Test_text_file.txt", polynomials, enc_channel= True,SNR = 5)
    )

# receiving
rec_message, channel_decode = receive(noisy_sig, huff_tree, polynomials)
hamming = ham_dis(entropy_code_ints, channel_decode)
rec_message

print("Hamming distance for SNR {} is {}, with BER: {}".format(5, hamming, hamming/len(entropy_code_ints)))

"""# Testing different values of SNR with/wihout channel coding
For this step, we will try different values of SNR and plot the bit error rate vs SNR with and without channel coding

### Creating the corresponding lists
"""

SNR_array = np.arange(0,15,1)

# with channel coding
Hamm_ch = []
rec_message_ch_list = []

# without channel coding
Hamm_no_ch = []
rec_message_no_ch_list = []

for snr in SNR_array:
  #************************ with channel coding******************
  # tx
  noisy_sig_ch, entropy_code_ints_ch, huff_tree_ch = (
      transmit("Test_text_file.txt", polynomials, SNR = snr)
      )
  # rx
  rec_message_ch, channel_decode_ch = (
      receive(noisy_sig_ch, huff_tree_ch,polynomials)
      )
  Hamm_ch.append(ham_dis(channel_decode_ch, entropy_code_ints_ch))
  rec_message_ch_list.append(rec_message_ch)

  ##******************** without channel coding************
  # tx
  noisy_sig_no_ch, entropy_code_ints_no_ch, huff_tree_no_ch = (
    transmit("Test_text_file.txt", polynomials, SNR = snr, enc_channel=False)
      )
  # rx
  rec_message_no_ch, channel_decode_no_ch = (
      receive(noisy_sig_no_ch, huff_tree_no_ch,polynomials, enc_channel=False)
      )
  Hamm_no_ch.append(ham_dis(channel_decode_no_ch, entropy_code_ints_no_ch))
  rec_message_no_ch_list.append(rec_message_no_ch)

"""### Plotting"""

# Commented out IPython magic to ensure Python compatibility.
import matplotlib.pyplot as plt
# %matplotlib inline

plt.plot(SNR_array, np.array(Hamm_ch)/len(entropy_code_ints));
plt.plot(SNR_array, np.array(Hamm_no_ch)/len(entropy_code_ints));
plt.yscale("log")
plt.legend(["Conv code", "No channel code"]);
plt.ylabel("BER (log scale)");
plt.xlabel("SNR (dB)");

"""### Conclusion

From the plots, it is obvious that
1. For all values of SNR > 1, BER with channel coding is much lower than that without channel coding
2. Only for really small SNR (0 dB or lower), not using convolutional code is better.

So, in conclusion, the convolutional code ahieves its purpose in **dramatically** lowering the BER without the need of enhancing the channel itself, but, at the cost of reduced effective rate, and some overhead in decoding

# Showing How the message is corrupted

### SNR = 5 dB

#### With channel coding
"""

rec_message_ch_list[5]

"""#### Without channel coding"""

rec_message_no_ch_list[5]

"""#### It's obvious that for the same SNR with channel coding, words are more comprehended than in the case of no channel coding. Nearly no word can be comprehended.

### SNR = 8 dB

#### With channel coding
"""

rec_message_ch_list[8]

"""#### Without Channel coding"""

rec_message_no_ch_list[8]

"""#### Nearly all characters in channel coding are cmpreheneded, yet for the case of no channel coding, most words are not comprehended

### SNR = 0 dB

#### With Channel Coding
"""

rec_message_ch_list[0]

"""#### Without Channel Coding

"""

rec_message_no_ch_list[0]

"""#### In both cases, no words are comprehended. the SNR is too low, (noise is too high) to be corrected even by the convolutional code <br>"""